{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JiangboWANGfr/ParttimeJOB/blob/main/DeepBIDchange.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "P9Q9hiHqiNDR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9Q9hiHqiNDR",
        "outputId": "92f26522-af9f-449b-ff59-7fd73077d278"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "WScI8t-XimLb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WScI8t-XimLb",
        "outputId": "eb274c61-b09f-4743-de02-c73ce6b0135d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/parttimeJOB/DeepBID\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/parttimeJOB/DeepBID/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "4YMBfZjO1Qlu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4YMBfZjO1Qlu",
        "outputId": "a9ff8b60-d749-4b7b-9663-5ce410961a8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Copy of DeepBIDchange (1).ipynb'   dataset\t\t  layers.py    'Untitled (1)'\n",
            "'Copy of DeepBIDchange.ipynb'\t    DeepBIDchange.ipynb   __pycache__   utils.py\n",
            "'Copy of DeepBID.ipynb'\t\t    DeepBID.ipynb\t  Untitled\n"
          ]
        }
      ],
      "source": [
        "# !pip uninstall torch torchvision torchaudio -y\n",
        "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\n",
        "# !apt-get install -y locales\n",
        "# !locale-gen en_US.UTF-8\n",
        "# !update-locale LANG=en_US.UTF-8\n",
        "# !export LANG=en_US.UTF-8\n",
        "# !export LC_ALL=en_US.UTF-\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "QWER2iFy48Gf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QWER2iFy48Gf",
        "outputId": "a3cc3e66-a2f1-4484-bfe9-2d72d5037178"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch version: 2.5.1+cu121\n",
            "Torchvision version: 0.20.1+cu121\n",
            "CUDA available: True\n",
            "CUDA version: 12.1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "print(\"Torchvision version:\", torchvision.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"CUDA version:\", torch.version.cuda)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.synchronize()\n",
        "torch.cuda.reset_max_memory_allocated()"
      ],
      "metadata": {
        "id": "psiVrCx91m1I"
      },
      "id": "psiVrCx91m1I",
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "b0b72ca7-39ff-4ecf-b9ee-e0062833673b",
      "metadata": {
        "id": "b0b72ca7-39ff-4ecf-b9ee-e0062833673b"
      },
      "outputs": [],
      "source": [
        "from multiprocessing import reduction\n",
        "import torch\n",
        "from os.path import join\n",
        "import numpy as np\n",
        "import csv\n",
        "import utils\n",
        "import pandas as pd\n",
        "#from metrics import cal_clustering_metric\n",
        "from scipy.sparse import coo_matrix\n",
        "from sklearn.cluster import KMeans\n",
        "import scipy.io as scio\n",
        "import random\n",
        "import warnings\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Parameter\n",
        "from time import time\n",
        "import torch.nn as nn\n",
        "from layers import NBLoss, MeanAct, DispAct\n",
        "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "7e075e90-0176-43a1-85b4-2231341dcdc8",
      "metadata": {
        "id": "7e075e90-0176-43a1-85b4-2231341dcdc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90904775-83b7-4e83-ac26-29112e8f305d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(569, 1000)\n",
            "(569, 1)\n",
            "(569, 1)\n"
          ]
        }
      ],
      "source": [
        "# raw_data = pd.read_csv(\"dataset/DC_pre.csv\")\n",
        "# BID_data = np.array(raw_data)\n",
        "# raw_label = pd.read_csv(\"dataset/DC_celltype.csv\")\n",
        "# BID_label = np.array(raw_label)\n",
        "# BID_labels = [int(x) for item in BID_label for x in item] #\n",
        "# raw_batch = pd.read_csv(\"dataset/DC_batch_1.csv\")\n",
        "# BID_batch = np.array(raw_batch)\n",
        "# BID_batches = [int(x) for item in BID_batch for x in item] #\n",
        "\n",
        "print(raw_data.shape)\n",
        "print(raw_label.shape)\n",
        "print(raw_batch.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "JIU25MLlJeyx",
      "metadata": {
        "id": "JIU25MLlJeyx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "b38edb7c-391a-4a70-f675-735a0896d52c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-58-531718c7006b>, line 5)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-58-531718c7006b>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    BID_labels = [int(x) for item in BID_label for x in item, usecols=[1]]\u001b[0m\n\u001b[0m                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "raw_data = pd.read_csv(\"dataset/Lung_counts.csv\")\n",
        "BID_data = np.array(raw_data)\n",
        "raw_label = pd.read_csv(\"dataset/Lung_celltype_codes.csv\"usecols=[1])\n",
        "BID_label = np.array(raw_label)\n",
        "BID_labels = [int(x) for item in BID_label for x in item]\n",
        "raw_batch = pd.read_csv(\"dataset/DC_batch_1.csv\"usecols=[1])\n",
        "BID_batch = np.array(raw_batch)\n",
        "BID_batches = [int(x) for item in BID_batch for x in item]\n",
        "print(raw_data.shape)\n",
        "print(raw_label.shape)\n",
        "print(raw_batch.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14a52a6e-e386-4dc6-a5b7-05c793de8801",
      "metadata": {
        "id": "14a52a6e-e386-4dc6-a5b7-05c793de8801"
      },
      "outputs": [],
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X):\n",
        "        self.X = X\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[:, idx], idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.shape[1]\n",
        "\n",
        "class Discriminator(torch.nn.Module):\n",
        "    def __init__(self, input_dim, num_batches):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_dim, 128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(128, 64),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(64, num_batches),  # 输出类别数 = 批次数\n",
        "            # torch.nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.model(z)\n",
        "\n",
        "class PretrainDoubleLayer(torch.nn.Module):\n",
        "    def __init__(self, X, dim, device, act, batch_size=128, lr=10**-5,layers=None):\n",
        "        super(PretrainDoubleLayer, self).__init__()\n",
        "        self.X = X\n",
        "        self.dim = dim\n",
        "        self.lr = lr\n",
        "        self.device = device\n",
        "        self.enc = torch.nn.Linear(X.shape[0], self.dim)\n",
        "        self.dec = torch.nn.Linear(self.dim, X.shape[0])\n",
        "        self.batch_size = batch_size\n",
        "        self.act = act\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.act is not None:\n",
        "            z = self.act(self.enc(x))\n",
        "            return z, self.act(self.dec(z))\n",
        "        else:\n",
        "            z = self.enc(x)\n",
        "            return z, self.dec(z)\n",
        "\n",
        "    def _build_loss(self, x, recons_x):\n",
        "        size = x.shape[0]\n",
        "        return torch.norm(x-recons_x, p='fro')**2 / size\n",
        "\n",
        "    def run(self):\n",
        "        self.to(self.device)\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
        "        train_loader = torch.utils.data.DataLoader(Dataset(self.X), batch_size=self.batch_size, shuffle=True)\n",
        "        loss = 0\n",
        "        for epoch in range(10):\n",
        "            for i, batch in enumerate(train_loader):\n",
        "                x, _ = batch\n",
        "                optimizer.zero_grad()\n",
        "                _, recons_x = self(x)\n",
        "                loss = self._build_loss(x, recons_x)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            print('epoch-{}: loss={}'.format(epoch, loss.item()))\n",
        "        Z, _ = self(self.X.t())\n",
        "        return Z.t()\n",
        "\n",
        "class DeepBID(torch.nn.Module):\n",
        "    def __init__(self, X, labels, your_batch , layers=None,lam1=1.0,lam2=0.001, sigma=None, gamma=1.0, lr=10**-3, kl1=1, kl2=1, nb=1, device=None, batch_size=128,\n",
        "                 torch_device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n",
        "        super(DeepBID, self).__init__()\n",
        "        if layers is None:\n",
        "            layers = [X.shape[0], 512, 300]\n",
        "        if device is None:\n",
        "            device = torch_device\n",
        "        self.layers = layers\n",
        "        self.count = 0\n",
        "        self.device = device\n",
        "        if not isinstance(X, torch.Tensor):\n",
        "            X = torch.Tensor(X)\n",
        "        # print(device)\n",
        "        # torch.cuda.synchronize()\n",
        "        # print(torch.cuda.memory_summary(device))\n",
        "        # try:\n",
        "        #   X = X.to(device)  # 逐步移动到 CUDA\n",
        "        #   print(\"X successfully moved to device:\", X.device)\n",
        "        # except RuntimeError as e:\n",
        "        #     print(\"Error moving X to device:\", e)\n",
        "\n",
        "        # print(X.dtype)  # 确保是 float 类型\n",
        "        # print(torch.isnan(X).any(), torch.isinf(X).any())  # 确保数据没有 NaN 或 Inf\n",
        "        # print(\"x shape \",X.shape)\n",
        "        self.X = X.to(device)\n",
        "        x_mean, x_std = torch.mean(self.X, dim=0), torch.std(self.X, dim=0)\n",
        "        x_norm = (self.X - x_mean) / x_std\n",
        "        self.X = x_norm\n",
        "        self.csv_batch = your_batch\n",
        "        self.n_batch = len(np.unique(self.csv_batch))\n",
        "        self.labels = labels\n",
        "        self.gamma = gamma\n",
        "        self.lam1 = lam1\n",
        "        self.lam2 = lam2\n",
        "        self.sigma = sigma\n",
        "        self.t_alpha = 1.0\n",
        "        self.kl1 = kl1\n",
        "        self.kl2 = kl2\n",
        "        self.nb = nb\n",
        "        self.batch_size = batch_size\n",
        "        self.n_clusters = len(np.unique(self.labels))\n",
        "        self.lr = lr\n",
        "        self.n = X.shape[1] // batch_size +2\n",
        "        self.encoder = torch.nn.Linear(self.layers[0], self.layers[1])\n",
        "        self.decoder = torch.nn.Linear(self.layers[1], self.layers[0])\n",
        "        # self.discriminator = BatchDiscriminator(feature_dim=self.layers[2]).to(self.device)\n",
        "        # self.d_optimizer = torch.optim.Adam(self.discriminator.parameters(), lr=lr)\n",
        "        self.discriminator = Discriminator(input_dim=self.layers[2], num_batches=self.n_batch).to(self.device)\n",
        "        self.d_optimizer = torch.optim.Adam(self.discriminator.parameters(), lr=self.lr)\n",
        "        self._dec_mean = torch.nn.Sequential(torch.nn.Linear(X.shape[0], X.shape[0]), MeanAct())\n",
        "        self._dec_disp = torch.nn.Sequential(torch.nn.Linear(X.shape[0], X.shape[0]), DispAct())\n",
        "        self.nb_loss = NBLoss().to(self.device)\n",
        "        self._build_up()\n",
        "\n",
        "    def _build_up(self):\n",
        "        self.act = torch.tanh\n",
        "        self.enc1 = torch.nn.Linear(self.layers[0], self.layers[1])\n",
        "        self.enc2 = torch.nn.Linear(self.layers[1], self.layers[2])\n",
        "        self.dec1 = torch.nn.Linear(self.layers[2], self.layers[1])\n",
        "        self.dec2 = torch.nn.Linear(self.layers[1], self.layers[0])\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.act(self.enc1(x))\n",
        "        z = self.act(self.enc2(z))\n",
        "        recons_x = self.act(self.dec1(z))\n",
        "        recons_x = self.act(self.dec2(recons_x))\n",
        "        return z, recons_x\n",
        "\n",
        "    def _build_loss(self, z, x, d, u, recons_x):\n",
        "        size = x.shape[0]\n",
        "        loss = 1/2 * torch.norm(x - recons_x, p='fro') ** 2 / size\n",
        "        t = d*u  # t: m * c\n",
        "        distances = utils.distance(z.t(), self.centroids)\n",
        "        loss = (self.lam1 / 2 * torch.trace(distances.t().matmul(t)) / size)\n",
        "        loss += self.lam2 * (self.enc1.weight.norm()**2 + self.enc1.bias.norm()**2) / size\n",
        "        loss += self.lam2 * (self.enc2.weight.norm()**2 + self.enc2.bias.norm()**2) / size\n",
        "        loss += self.lam2 * (self.dec1.weight.norm()**2 + self.dec1.bias.norm()**2) / size\n",
        "        loss += self.lam2 * (self.dec2.weight.norm()**2 + self.dec2.bias.norm()**2) / size\n",
        "        return loss\n",
        "\n",
        "    def cul_batch_kl(self,csv_batch,cu):\n",
        "        self.to(self.device)\n",
        "        csv_batch = np.array(csv_batch)\n",
        "        bt = torch.from_numpy(csv_batch)\n",
        "        bt_count = torch.bincount(bt)\n",
        "        qt = bt_count/bt.numel()\n",
        "        B = torch.Tensor(self.n_clusters,bt.numel()) #4*5\n",
        "        B.copy_(bt)\n",
        "        B = B.t()\n",
        "        B = B.to(self.device)\n",
        "        Z = torch.zeros(bt.numel(),self.n_clusters).to(self.device)\n",
        "        su = self.U.sum(axis = 0)\n",
        "        tensor_list = list()\n",
        "        for i in range(0,self.n_batch):\n",
        "            u0 = torch.where(B==i,cu,Z)\n",
        "            su0 = u0.sum(axis = 0)\n",
        "            pb0 = torch.div(su0,su)\n",
        "            tensor_list.append(pb0)\n",
        "        pb = torch.stack(tensor_list)\n",
        "        Q = torch.Tensor(self.n_clusters,qt.numel())\n",
        "        Q = Q.copy_(qt).t().to(self.device)\n",
        "        kl = pb * torch.log(torch.div(pb+1e-6,Q+1e-6))\n",
        "        kl_sum = kl.sum().item()\n",
        "        return kl_sum\n",
        "\n",
        "    def target_distribution(self, q):\n",
        "        p = q**2 / q.sum(0)\n",
        "        return torch.div(p.t(),p.sum(1)+1).t()\n",
        "\n",
        "    def cal_latent(self,hidden, alpha):\n",
        "        sum_y = torch.sum(torch.square(hidden), axis=1)\n",
        "        num = -2.0 *torch.mm(hidden, hidden.t()) + torch.reshape(sum_y, [-1, 1]) + sum_y\n",
        "        num = num / alpha\n",
        "        num = torch.pow(1.0 + num, -(alpha + 1.0) / 2.0)\n",
        "        zerodiag_num = num - torch.diag(torch.diag(num))\n",
        "        latent_p = (zerodiag_num.t() / torch.sum(zerodiag_num, axis=1)).t()\n",
        "        return num, latent_p\n",
        "\n",
        "    def target_dis(self,latent_p):\n",
        "        latent_q = torch.pow(latent_p, 2).t() / torch.sum(latent_p, axis = 1)\n",
        "        latent_q = latent_q.t()\n",
        "        return (latent_q.t() / torch.sum(latent_q, axis = 1)).t()\n",
        "\n",
        "    def forwardAE(self, x):\n",
        "        x=x.T\n",
        "        h = self.act(self.encoder(x))\n",
        "        h = self.act(self.decoder(h))\n",
        "        _mean = self._dec_mean(h)\n",
        "        _disp = self._dec_disp(h)\n",
        "        return _mean, _disp\n",
        "\n",
        "\n",
        "    def run(self):\n",
        "        self.to(self.device)\n",
        "        self.pretrain()\n",
        "        Z, _ = self(self.X.t())\n",
        "        Z = Z.t().detach()\n",
        "        idx = random.sample(list(range(Z.shape[0])), self.n_clusters)\n",
        "        self.centroids = Z[:, idx] + 10 ** -8\n",
        "        self._update_U(Z)\n",
        "        print('Starting training......')\n",
        "        train_loader = torch.utils.data.DataLoader(Dataset(self.X), batch_size=self.batch_size, shuffle=True)\n",
        "        optimizer = torch.optim.SGD(self.parameters(),lr = self.lr)\n",
        "        loss = 0\n",
        "        t0 = time()\n",
        "        cu = self.U[self.n_clusters, :]\n",
        "        index = []\n",
        "        for epoch in range(25):\n",
        "            D = self._update_D(Z)\n",
        "            for i, batch in enumerate(train_loader):\n",
        "                x, idx = batch\n",
        "                idx_n = idx.numpy()\n",
        "                index.extend(idx_n)\n",
        "                optimizer.zero_grad()\n",
        "                z, recons_x = self(x)\n",
        "                d = D[idx, :]\n",
        "                u = self.U[idx, :]\n",
        "                self.loss = self._build_loss(z, x, d, u, recons_x)\n",
        "\n",
        "                # 计算负二项分布损失\n",
        "                _, y_pred = self.U.max(dim=1)\n",
        "                meanbatch, dispbatch= self.forwardAE(self.X)\n",
        "                input = self.X.T\n",
        "                self.loss_nb = self.nb_loss(input,meanbatch, dispbatch)\n",
        "                self.loss_nb = self.nb * self.loss_nb\n",
        "                #计算 KL 散度损失\n",
        "                self.num, self.latent_p = self.cal_latent(recons_x, self.t_alpha)\n",
        "                self.latent_q = self.target_dis(self.latent_p)\n",
        "                self.latent_p = self.latent_p + torch.diag(torch.diag(self.num))\n",
        "                self.latent_q = self.latent_q + torch.diag(torch.diag(self.num))\n",
        "                self.cross_entropy = -torch.sum(self.latent_q * torch.log(self.latent_p))\n",
        "                self.entropy = -torch.sum(self.latent_q * torch.log(self.latent_q))\n",
        "                self.loss_kl1 = self.cross_entropy - self.entropy\n",
        "                self.loss_kl1 = self.kl1 * self.loss_kl1\n",
        "                # self.loss_kl2 = self.cul_batch_kl(self.csv_batch,cu)\n",
        "                # self.loss_kl2 = self.kl2 * self.loss_kl2\n",
        "\n",
        "                # ========== GAN  ==========\n",
        "                self.d_optimizer.zero_grad()\n",
        "                batch_pred = self.discriminator(z.detach())\n",
        "                batch_labels = torch.tensor(np.array(self.csv_batch)[idx.cpu().numpy()], dtype=torch.long).to(self.device)\n",
        "                loss_d = torch.nn.CrossEntropyLoss()(batch_pred, batch_labels)  # 判别器的分类损失\n",
        "                loss_d.backward()\n",
        "                self.d_optimizer.step()\n",
        "\n",
        "\n",
        "                batch_pred_fake = self.discriminator(z)\n",
        "                target_distribution = torch.ones_like(batch_pred_fake) / self.n_batch\n",
        "                self.loss_g = torch.nn.functional.kl_div(torch.nn.functional.log_softmax(batch_pred_fake, dim=1),\n",
        "                                                        target_distribution,\n",
        "                                                        reduction='batchmean')\n",
        "\n",
        "                # ========== 总损失（GAN + 其他损失） ==========\n",
        "                self.loss_sum = self.loss + self.loss_kl1 + self.loss_g + self.loss_nb\n",
        "                self.loss_sum.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            # 更新聚类\n",
        "            Z, _ = self(self.X.t())\n",
        "            Z = Z.t().detach()\n",
        "\n",
        "            # 在每个epoch结束后，保存当前的嵌入层\n",
        "            self.embedding = Z.cpu().numpy() # 将embedding移动到CPU并转换为numpy数组\n",
        "\n",
        "            self.clustering(Z, 1)\n",
        "            _, y_pred = self.U.max(dim=1)\n",
        "            y_pred = y_pred.detach().cpu() + 1\n",
        "            y_pred = y_pred.numpy()\n",
        "            self.ARI = np.around(adjusted_rand_score(self.labels, y_pred),6)\n",
        "            self.NMI = np.around(normalized_mutual_info_score(self.labels, y_pred), 6)\n",
        "            print('epoch-{}, loss_sum={:.5f},loss={:.5f},loss_kl1={:.5f},loss_g={:.5f},loss_nb={:.5f},NMI={:.5f}, ARI={:.5f}'.format(epoch, self.loss_sum,self.loss,self.loss_kl1,self.loss_g,self.loss_nb,  self.NMI,self.ARI))\n",
        "\n",
        "        # 训练结束后，保存最终的嵌入层\n",
        "        self.final_embedding = self.embedding\n",
        "\n",
        "    def pretrain(self):\n",
        "        string_template = 'Start pretraining-{}......'\n",
        "        print(string_template.format(1))\n",
        "        pre1 = PretrainDoubleLayer(self.X, self.layers[1], self.device, self.act, lr=self.lr)\n",
        "        Z = pre1.run()\n",
        "        self.enc1.weight = pre1.enc.weight\n",
        "        self.enc1.bias = pre1.enc.bias\n",
        "        self.dec2.weight = pre1.dec.weight\n",
        "        self.dec2.bias = pre1.dec.bias\n",
        "        print(string_template.format(2))\n",
        "\n",
        "        pre2 = PretrainDoubleLayer(Z.detach(), self.layers[2], self.device, self.act, lr=self.lr)\n",
        "        Z2 = pre2.run()  # Capture the output of pre2.run()\n",
        "        self.enc2.weight = pre2.enc.weight\n",
        "        self.enc2.bias = pre2.enc.bias\n",
        "        self.dec1.weight = pre2.dec.weight\n",
        "        self.dec1.bias = pre2.dec.bias\n",
        "\n",
        "    def _update_D(self, Z):\n",
        "        if self.sigma is None:\n",
        "            return torch.ones([Z.shape[1], self.centroids.shape[1]]).to(self.device)\n",
        "        else:\n",
        "            distances = utils.distance(Z, self.centroids, False)\n",
        "            return (1 + self.sigma) * (distances + 2 * self.sigma) / (2 * (distances + self.sigma))\n",
        "\n",
        "    def clustering(self, Z, max_iter=1):\n",
        "        for i in range(max_iter):\n",
        "            D = self._update_D(Z)\n",
        "            T = D * self.U\n",
        "            self.centroids = Z.matmul(T) / T.sum(dim=0).reshape([1, -1])\n",
        "            self._update_U(Z)\n",
        "\n",
        "    def _update_U(self, Z):\n",
        "        if self.sigma is None:\n",
        "            distances = utils.distance(Z, self.centroids, False)\n",
        "        else:\n",
        "            distances = self.adaptive_loss(utils.distance(Z, self.centroids, False), self.sigma)\n",
        "        U = torch.exp(-distances / self.gamma)\n",
        "        self.U = U / U.sum(dim=1).reshape([-1, 1])\n",
        "\n",
        "    def adaptive_loss(self,D, sigma):\n",
        "        return (1 + sigma) * D * D / (D + sigma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ace44a66-9132-4149-973e-661d90de6e60",
      "metadata": {
        "collapsed": true,
        "id": "ace44a66-9132-4149-973e-661d90de6e60",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    data, labels ,csv_batch = BID_data, BID_labels,BID_batches\n",
        "    data = data.T\n",
        "    for lam1 in [0.01,0.05,0.1,0.5,1,5,10]:\n",
        "        print('lam1={}'.format(lam1))\n",
        "        bid = DeepBID(data, labels, BID_batches,[data.shape[0],1000, 500, 200], lam1 = lam1,lam2=0.001,gamma=1,sigma=1,kl1=0.1, kl2=0.1, nb=1,  batch_size=128, lr=10**-5)\n",
        "        bid.run()\n",
        "        embeddings = bid.final_embedding #获取最终的embedding\n",
        "        batches = np.array(BID_batches)\n",
        "        #现在就可以用embeddings和batches来做umap绘图分析批次效应了\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a22c1281-554c-4db2-a706-14cb9d53ace0",
      "metadata": {
        "id": "a22c1281-554c-4db2-a706-14cb9d53ace0"
      },
      "outputs": [],
      "source": [
        "embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!pip install scanpy"
      ],
      "metadata": {
        "id": "EvgUMlfuzkeQ"
      },
      "id": "EvgUMlfuzkeQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14c91a7b-af89-40d5-9cfd-e243eb3b59a1",
      "metadata": {
        "id": "14c91a7b-af89-40d5-9cfd-e243eb3b59a1"
      },
      "outputs": [],
      "source": [
        "import scanpy as sc\n",
        "batches = np.array(BID_batches)\n",
        "cell_types = np.array(labels)  # 假设 labels 包含了细胞类型信息\n",
        "\n",
        "adata = sc.AnnData(data.T)  # 将原始数据放入X\n",
        "adata.obs['batch'] = pd.Categorical(batches)  # 批次信息\n",
        "adata.obs['cell_type'] = pd.Categorical(cell_types)  # 细胞类型信息\n",
        "adata.obsm['X_emb'] = embeddings.T # 将embeddings放入到obsm中\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26365ae9-5690-4984-b7f4-cdfe9fb768ff",
      "metadata": {
        "id": "26365ae9-5690-4984-b7f4-cdfe9fb768ff"
      },
      "outputs": [],
      "source": [
        "sc.pp.neighbors(adata, use_rep='X_emb')\n",
        "sc.tl.umap(adata)\n",
        "sc.pl.umap(adata, color=['batch', 'cell_type'], title=\"UMAP after Correction with GAN\")  # 观察批次效应是否减少"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4yXikPOnnulN",
      "metadata": {
        "id": "4yXikPOnnulN"
      },
      "source": [
        "上面的方法就是去除了批次效应，左侧的标签是批次，右侧的标签是细胞类型，左侧不同的批次混合在一起，右侧细胞聚类也尽可能聚在一起，可以与下面的未去除批次效应相比较"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e861d3e-935b-485a-a708-cdc3fe055dd5",
      "metadata": {
        "id": "8e861d3e-935b-485a-a708-cdc3fe055dd5"
      },
      "outputs": [],
      "source": [
        "import scanpy as sc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scanpy as sc\n",
        "raw_data = pd.read_csv(\"dataset/DC_pre.csv\")\n",
        "BID_data = np.array(raw_data)\n",
        "raw_label = pd.read_csv(\"dataset/DC_celltype.csv\")\n",
        "BID_label = np.array(raw_label)\n",
        "BID_labels = [int(x) for item in BID_label for x in item] #\n",
        "raw_batch = pd.read_csv(\"dataset/DC_batch_1.csv\")\n",
        "BID_batch = np.array(raw_batch)\n",
        "BID_batches = [int(x) for item in BID_batch for x in item] #\n",
        "batches = np.array(BID_batches)\n",
        "\n",
        "data, labels ,csv_batch = BID_data, BID_labels,BID_batches\n",
        "cell_types = np.array(labels)  # 假设 labels 包含了细胞类型信息\n",
        "adata = sc.AnnData(data)  # 将原始数据放入X\n",
        "adata.obs['batch'] = pd.Categorical(batches)  # 批次信息\n",
        "adata.obs['cell_type'] = pd.Categorical(labels)  # 细胞类型信息\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4469aadf-a1f3-4807-89c8-907fd6845b2f",
      "metadata": {
        "id": "4469aadf-a1f3-4807-89c8-907fd6845b2f"
      },
      "outputs": [],
      "source": [
        "adata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdbe8465-041b-4c2e-b663-9e0d02cef73d",
      "metadata": {
        "id": "bdbe8465-041b-4c2e-b663-9e0d02cef73d"
      },
      "outputs": [],
      "source": [
        "sc.pp.pca(adata)\n",
        "sc.pp.neighbors(adata,n_neighbors=10,n_pcs=25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e53dae06-7e27-4d15-b096-3ca4d6e51277",
      "metadata": {
        "id": "e53dae06-7e27-4d15-b096-3ca4d6e51277"
      },
      "outputs": [],
      "source": [
        "sc.tl.umap(adata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yjRemH5Nym-j",
      "metadata": {
        "collapsed": true,
        "id": "yjRemH5Nym-j"
      },
      "outputs": [],
      "source": [
        "!pip3 install igraph\n",
        "!pip3 install leidenalg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11444f9d-4711-4b0d-a0a0-da61b825bd2f",
      "metadata": {
        "id": "11444f9d-4711-4b0d-a0a0-da61b825bd2f"
      },
      "outputs": [],
      "source": [
        "sc.tl.leiden(adata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9707cdc0-66b7-4723-9389-ee9913d51375",
      "metadata": {
        "id": "9707cdc0-66b7-4723-9389-ee9913d51375"
      },
      "outputs": [],
      "source": [
        "sc.pl.umap(adata, color=['batch', 'cell_type'], title=\"UMAP before Correction\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EzwE31rE0TNE",
      "metadata": {
        "id": "EzwE31rE0TNE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}